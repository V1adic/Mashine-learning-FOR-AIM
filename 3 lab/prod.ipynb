{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', '+', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'h', 't', 'times', 'w', 'X', 'y']\n",
      "X_train: torch.Size([10, 32, 32]) type: torch.FloatTensor\n",
      "y_train: torch.Size([10, 21]) type: torch.FloatTensor\n",
      "Train Epoch: 0 [0/63 (0%)]\tLoss: 3.075212 Epoch_Loss: 3.075212\n",
      "Train Epoch: 1 [0/63 (0%)]\tLoss: 3.033164 Epoch_Loss: 3.033164\n",
      "Train Epoch: 2 [0/63 (0%)]\tLoss: 3.019285 Epoch_Loss: 3.019285\n",
      "Train Epoch: 3 [0/63 (0%)]\tLoss: 2.994700 Epoch_Loss: 2.994700\n",
      "Train Epoch: 4 [0/63 (0%)]\tLoss: 3.009886 Epoch_Loss: 3.009886\n",
      "Train Epoch: 5 [0/63 (0%)]\tLoss: 2.942302 Epoch_Loss: 2.942302\n",
      "Train Epoch: 6 [0/63 (0%)]\tLoss: 2.927117 Epoch_Loss: 2.927117\n",
      "Train Epoch: 7 [0/63 (0%)]\tLoss: 2.931708 Epoch_Loss: 2.931708\n",
      "Train Epoch: 8 [0/63 (0%)]\tLoss: 2.937812 Epoch_Loss: 2.937812\n",
      "Train Epoch: 9 [0/63 (0%)]\tLoss: 2.859633 Epoch_Loss: 2.859633\n",
      "Train Epoch: 10 [0/63 (0%)]\tLoss: 2.815359 Epoch_Loss: 2.815359\n",
      "Train Epoch: 11 [0/63 (0%)]\tLoss: 2.893573 Epoch_Loss: 2.893573\n",
      "Train Epoch: 12 [0/63 (0%)]\tLoss: 2.818259 Epoch_Loss: 2.818259\n",
      "Train Epoch: 13 [0/63 (0%)]\tLoss: 2.765612 Epoch_Loss: 2.765612\n",
      "Train Epoch: 14 [0/63 (0%)]\tLoss: 2.745411 Epoch_Loss: 2.745411\n",
      "Train Epoch: 15 [0/63 (0%)]\tLoss: 2.712003 Epoch_Loss: 2.712003\n",
      "Train Epoch: 16 [0/63 (0%)]\tLoss: 2.746543 Epoch_Loss: 2.746543\n",
      "Train Epoch: 17 [0/63 (0%)]\tLoss: 2.704791 Epoch_Loss: 2.704791\n",
      "Train Epoch: 18 [0/63 (0%)]\tLoss: 2.708076 Epoch_Loss: 2.708076\n",
      "Train Epoch: 19 [0/63 (0%)]\tLoss: 2.710106 Epoch_Loss: 2.710106\n",
      "Train Epoch: 20 [0/63 (0%)]\tLoss: 2.709507 Epoch_Loss: 2.709507\n",
      "Train Epoch: 21 [0/63 (0%)]\tLoss: 2.613551 Epoch_Loss: 2.613551\n",
      "Train Epoch: 22 [0/63 (0%)]\tLoss: 2.766468 Epoch_Loss: 2.766468\n",
      "Train Epoch: 23 [0/63 (0%)]\tLoss: 2.610274 Epoch_Loss: 2.610274\n",
      "Train Epoch: 24 [0/63 (0%)]\tLoss: 2.633286 Epoch_Loss: 2.633286\n",
      "Train Epoch: 25 [0/63 (0%)]\tLoss: 2.699627 Epoch_Loss: 2.699627\n",
      "Train Epoch: 26 [0/63 (0%)]\tLoss: 2.497296 Epoch_Loss: 2.497296\n",
      "Train Epoch: 27 [0/63 (0%)]\tLoss: 2.477215 Epoch_Loss: 2.477215\n",
      "Train Epoch: 28 [0/63 (0%)]\tLoss: 2.551104 Epoch_Loss: 2.551104\n",
      "Train Epoch: 29 [0/63 (0%)]\tLoss: 2.375179 Epoch_Loss: 2.375179\n",
      "Train Epoch: 30 [0/63 (0%)]\tLoss: 2.448250 Epoch_Loss: 2.448250\n",
      "Train Epoch: 31 [0/63 (0%)]\tLoss: 2.239424 Epoch_Loss: 2.239424\n",
      "Train Epoch: 32 [0/63 (0%)]\tLoss: 2.267301 Epoch_Loss: 2.267301\n",
      "Train Epoch: 33 [0/63 (0%)]\tLoss: 2.446084 Epoch_Loss: 2.446084\n",
      "Train Epoch: 34 [0/63 (0%)]\tLoss: 2.432566 Epoch_Loss: 2.432566\n",
      "Train Epoch: 35 [0/63 (0%)]\tLoss: 2.474286 Epoch_Loss: 2.474286\n",
      "Train Epoch: 36 [0/63 (0%)]\tLoss: 2.266569 Epoch_Loss: 2.266569\n",
      "Train Epoch: 37 [0/63 (0%)]\tLoss: 2.271505 Epoch_Loss: 2.271505\n",
      "Train Epoch: 38 [0/63 (0%)]\tLoss: 2.209049 Epoch_Loss: 2.209049\n",
      "Train Epoch: 39 [0/63 (0%)]\tLoss: 2.129258 Epoch_Loss: 2.129258\n",
      "Train Epoch: 40 [0/63 (0%)]\tLoss: 2.174368 Epoch_Loss: 2.174368\n",
      "Train Epoch: 41 [0/63 (0%)]\tLoss: 2.403302 Epoch_Loss: 2.403302\n",
      "Train Epoch: 42 [0/63 (0%)]\tLoss: 1.945487 Epoch_Loss: 1.945487\n",
      "Train Epoch: 43 [0/63 (0%)]\tLoss: 2.154742 Epoch_Loss: 2.154742\n",
      "Train Epoch: 44 [0/63 (0%)]\tLoss: 1.966020 Epoch_Loss: 1.966020\n",
      "Train Epoch: 45 [0/63 (0%)]\tLoss: 2.120046 Epoch_Loss: 2.120046\n",
      "Train Epoch: 46 [0/63 (0%)]\tLoss: 1.718211 Epoch_Loss: 1.718211\n",
      "Train Epoch: 47 [0/63 (0%)]\tLoss: 2.159140 Epoch_Loss: 2.159140\n",
      "Train Epoch: 48 [0/63 (0%)]\tLoss: 1.828661 Epoch_Loss: 1.828661\n",
      "Train Epoch: 49 [0/63 (0%)]\tLoss: 1.804684 Epoch_Loss: 1.804684\n",
      "Train Epoch: 50 [0/63 (0%)]\tLoss: 1.621046 Epoch_Loss: 1.621046\n",
      "Train Epoch: 51 [0/63 (0%)]\tLoss: 1.807980 Epoch_Loss: 1.807980\n",
      "Train Epoch: 52 [0/63 (0%)]\tLoss: 2.119720 Epoch_Loss: 2.119720\n",
      "Train Epoch: 53 [0/63 (0%)]\tLoss: 1.972822 Epoch_Loss: 1.972822\n",
      "Train Epoch: 54 [0/63 (0%)]\tLoss: 1.840212 Epoch_Loss: 1.840212\n",
      "Train Epoch: 55 [0/63 (0%)]\tLoss: 1.900757 Epoch_Loss: 1.900757\n",
      "Train Epoch: 56 [0/63 (0%)]\tLoss: 1.933161 Epoch_Loss: 1.933161\n",
      "Train Epoch: 57 [0/63 (0%)]\tLoss: 1.557490 Epoch_Loss: 1.557490\n",
      "Train Epoch: 58 [0/63 (0%)]\tLoss: 1.730509 Epoch_Loss: 1.730509\n",
      "Train Epoch: 59 [0/63 (0%)]\tLoss: 1.532662 Epoch_Loss: 1.532662\n",
      "Train Epoch: 60 [0/63 (0%)]\tLoss: 1.695604 Epoch_Loss: 1.695604\n",
      "Train Epoch: 61 [0/63 (0%)]\tLoss: 1.710703 Epoch_Loss: 1.710703\n",
      "Train Epoch: 62 [0/63 (0%)]\tLoss: 1.513011 Epoch_Loss: 1.513011\n",
      "Train Epoch: 63 [0/63 (0%)]\tLoss: 1.241858 Epoch_Loss: 1.241858\n",
      "Train Epoch: 64 [0/63 (0%)]\tLoss: 1.403138 Epoch_Loss: 1.403138\n",
      "Train Epoch: 65 [0/63 (0%)]\tLoss: 1.581292 Epoch_Loss: 1.581292\n",
      "Train Epoch: 66 [0/63 (0%)]\tLoss: 0.983668 Epoch_Loss: 0.983668\n",
      "Train Epoch: 67 [0/63 (0%)]\tLoss: 1.560883 Epoch_Loss: 1.560883\n",
      "Train Epoch: 68 [0/63 (0%)]\tLoss: 1.124049 Epoch_Loss: 1.124049\n",
      "Train Epoch: 69 [0/63 (0%)]\tLoss: 1.423893 Epoch_Loss: 1.423893\n",
      "Train Epoch: 70 [0/63 (0%)]\tLoss: 1.357988 Epoch_Loss: 1.357988\n",
      "Train Epoch: 71 [0/63 (0%)]\tLoss: 1.131466 Epoch_Loss: 1.131466\n",
      "Train Epoch: 72 [0/63 (0%)]\tLoss: 1.488858 Epoch_Loss: 1.488858\n",
      "Train Epoch: 73 [0/63 (0%)]\tLoss: 1.035491 Epoch_Loss: 1.035491\n",
      "Train Epoch: 74 [0/63 (0%)]\tLoss: 1.095754 Epoch_Loss: 1.095754\n",
      "Train Epoch: 75 [0/63 (0%)]\tLoss: 1.187409 Epoch_Loss: 1.187409\n",
      "Train Epoch: 76 [0/63 (0%)]\tLoss: 0.989913 Epoch_Loss: 0.989913\n",
      "Train Epoch: 77 [0/63 (0%)]\tLoss: 0.718574 Epoch_Loss: 0.718574\n",
      "Train Epoch: 78 [0/63 (0%)]\tLoss: 1.261581 Epoch_Loss: 1.261581\n",
      "Train Epoch: 79 [0/63 (0%)]\tLoss: 0.868899 Epoch_Loss: 0.868899\n",
      "Train Epoch: 80 [0/63 (0%)]\tLoss: 0.824162 Epoch_Loss: 0.824162\n",
      "Train Epoch: 81 [0/63 (0%)]\tLoss: 1.003367 Epoch_Loss: 1.003367\n",
      "Train Epoch: 82 [0/63 (0%)]\tLoss: 1.204532 Epoch_Loss: 1.204532\n",
      "Train Epoch: 83 [0/63 (0%)]\tLoss: 1.255457 Epoch_Loss: 1.255457\n",
      "Train Epoch: 84 [0/63 (0%)]\tLoss: 0.986882 Epoch_Loss: 0.986882\n",
      "Train Epoch: 85 [0/63 (0%)]\tLoss: 0.926503 Epoch_Loss: 0.926503\n",
      "Train Epoch: 86 [0/63 (0%)]\tLoss: 0.694575 Epoch_Loss: 0.694575\n",
      "Train Epoch: 87 [0/63 (0%)]\tLoss: 0.508384 Epoch_Loss: 0.508384\n",
      "Train Epoch: 88 [0/63 (0%)]\tLoss: 0.830036 Epoch_Loss: 0.830036\n",
      "Train Epoch: 89 [0/63 (0%)]\tLoss: 1.015858 Epoch_Loss: 1.015858\n",
      "Train Epoch: 90 [0/63 (0%)]\tLoss: 0.680648 Epoch_Loss: 0.680648\n",
      "Train Epoch: 91 [0/63 (0%)]\tLoss: 0.653531 Epoch_Loss: 0.653531\n",
      "Train Epoch: 92 [0/63 (0%)]\tLoss: 0.852162 Epoch_Loss: 0.852162\n",
      "Train Epoch: 93 [0/63 (0%)]\tLoss: 0.799689 Epoch_Loss: 0.799689\n",
      "Train Epoch: 94 [0/63 (0%)]\tLoss: 0.649334 Epoch_Loss: 0.649334\n",
      "Train Epoch: 95 [0/63 (0%)]\tLoss: 0.622263 Epoch_Loss: 0.622263\n",
      "Train Epoch: 96 [0/63 (0%)]\tLoss: 0.405433 Epoch_Loss: 0.405433\n",
      "Train Epoch: 97 [0/63 (0%)]\tLoss: 0.569607 Epoch_Loss: 0.569607\n",
      "Train Epoch: 98 [0/63 (0%)]\tLoss: 0.472336 Epoch_Loss: 0.472336\n",
      "Train Epoch: 99 [0/63 (0%)]\tLoss: 0.687894 Epoch_Loss: 0.687894\n",
      "Train Epoch: 100 [0/63 (0%)]\tLoss: 0.492071 Epoch_Loss: 0.492071\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io,morphology\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from random import shuffle\n",
    "import cv2 \n",
    "\n",
    "class NumericDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, img_size, num_classes, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_size = img_size # !!! внимательно при изменении размеров изображения\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        file_names = []\n",
    "        class_labels = []\n",
    "        for path, subdirs, files in os.walk(path_to_folder):\n",
    "            for (idx,name) in enumerate(files):\n",
    "                if(idx < MAX_FILES):\n",
    "                    file_names.append(os.path.join(path, name))\n",
    "                    class_labels.append(dict_folders[path.split('\\\\')[-1]])\n",
    "        self.files = [[file_names[i],class_labels[i]] for i in range(len(file_names))]#!!!\n",
    "        shuffle(self.files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx][0]\n",
    "\n",
    "        img = cv2.imread(img_name, cv2.COLOR_RGB2GRAY)\n",
    "        image = img\n",
    "        image = cv2.resize(image,(self.img_size, self.img_size))\n",
    "        image = cv2.bitwise_not(image) / 255\n",
    "        image = np.asarray(image).astype(float)#.reshape(3,self.img_size,self.img_size)\n",
    "\n",
    "        target = [0 for i in range(self.num_classes)]\n",
    "        target[self.files[idx][1]] = 1\n",
    "        target = torch.FloatTensor(target)\n",
    "        \n",
    "        image = torch.FloatTensor(image[:,:,0])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image,target,self.files[idx][1]\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMAGE_SIZE*IMAGE_SIZE, 4*IMAGE_SIZE*IMAGE_SIZE)\n",
    "        #self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(4*IMAGE_SIZE*IMAGE_SIZE, 4*NUM_CLASSES)\n",
    "        #self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(4*NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IMAGE_SIZE*IMAGE_SIZE)\n",
    "        #x = F.sigmoid(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fc1_drop(x)\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc2_drop(x)\n",
    "        #return F.softmax(self.fc3(x), dim=1)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    k = 0    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target,idx_class) in enumerate(num_train_dataloader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        #print(output,target)\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "        k+=1\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step() \n",
    "      \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Epoch_Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(num_train_dataloader.dataset),\n",
    "                100. * batch_idx / len(num_train_dataloader), loss.data.item(), epoch_loss))\n",
    "    return epoch_loss / k\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "path_to_folder = '.\\data' # путь к папке с данными\n",
    "name_folders = [x[0].split('\\\\')[-1] for x in os.walk(path_to_folder)]\n",
    "name_folders = name_folders[1:]\n",
    "print(name_folders)\n",
    "NUM_CLASSES = len(name_folders)\n",
    "MAX_FILES = 3\n",
    "\n",
    "class_idx = [i for i in range(len(name_folders))]\n",
    "dict_folders = {name_folders[i]:class_idx[i] for i in range(len(class_idx))}\n",
    "\n",
    "file_names = []\n",
    "class_labels = []\n",
    "for path, subdirs, files in os.walk(path_to_folder):\n",
    "    for name in files:\n",
    "        file_names.append(os.path.join(path, name))\n",
    "        class_labels.append(dict_folders[path.split('\\\\')[-1]])\n",
    "\n",
    "batch_size = 10\n",
    "IMAGE_SIZE = 32\n",
    "num_train_dataloader = DataLoader(NumericDataset(path_to_folder,IMAGE_SIZE,NUM_CLASSES), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for (X_train, y_train,class_idx) in num_train_dataloader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "\n",
    "epoch_start = 0\n",
    "epochs = 100\n",
    "path_model_save = './models/'\n",
    "\n",
    "model = Net().to(device) #!!!\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "lossv = []\n",
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    lossv.append(train(epoch))\n",
    "    \n",
    "torch.save(model.state_dict(), path_model_save+'mlp_model_22_classes_'+str(epoch)+'.pth')\n",
    "    #validate(lossv, accv)\n",
    "    #train(epoch, model_CNN)\n",
    "    #validate(lossv_CNN, accv_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, 'h': 15, 't': 16, 'times': 17, 'w': 18, 'X': 19, 'y': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnori\\AppData\\Local\\Temp\\ipykernel_16628\\2321144821.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  my_model.load_state_dict(torch.load('./models/mlp_model_22_classes_1000.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10+3-6=7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io,morphology\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from random import shuffle\n",
    "import cv2 \n",
    "\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMAGE_SIZE*IMAGE_SIZE, 4*IMAGE_SIZE*IMAGE_SIZE)\n",
    "        self.fc2 = nn.Linear(4*IMAGE_SIZE*IMAGE_SIZE, 4*NUM_CLASSES)\n",
    "        self.fc3 = nn.Linear(4*NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IMAGE_SIZE*IMAGE_SIZE)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "def GetItem(y):\n",
    "\n",
    "    img_name = \"\"\n",
    "    for i in range(len(files)):\n",
    "        if files[i][1] == y:\n",
    "            idx = i\n",
    "            break\n",
    "\n",
    "    img_name = files[idx][0]\n",
    "    img = cv2.imread(img_name, cv2.COLOR_RGB2GRAY)\n",
    "    image = img\n",
    "    image = cv2.resize(image,(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    image = cv2.bitwise_not(image) / 255\n",
    "    image = np.asarray(image).astype(float)#.reshape(3,self.img_size,self.img_size)\n",
    "\n",
    "    target = [0 for i in range(NUM_CLASSES)]\n",
    "    target[files[idx][1]] = 1\n",
    "    target = torch.FloatTensor(target)\n",
    "    \n",
    "    image = torch.FloatTensor(image[:,:,0])\n",
    "\n",
    "    return image,target,files[idx][1]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "path_to_folder = '.\\data' # путь к папке с данными\n",
    "name_folders = [x[0].split('\\\\')[-1] for x in os.walk(path_to_folder)]\n",
    "name_folders = name_folders[1:]\n",
    "\n",
    "NUM_CLASSES = len(name_folders)\n",
    "MAX_FILES = 3\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "class_idx = [i for i in range(len(name_folders))]\n",
    "dict_folders = {name_folders[i]:class_idx[i] for i in range(len(class_idx))}\n",
    "print(dict_folders)\n",
    "\n",
    "file_names = []\n",
    "class_labels = []\n",
    "for path, subdirs, files in os.walk(path_to_folder):\n",
    "    for (idx,name) in enumerate(files):\n",
    "        if(idx < MAX_FILES):\n",
    "            file_names.append(os.path.join(path, name))\n",
    "            class_labels.append(dict_folders[path.split('\\\\')[-1]])\n",
    "\n",
    "files = [[file_names[i],class_labels[i]] for i in range(len(file_names))]\n",
    "\n",
    "my_model = Net().to(device)\n",
    "my_model.load_state_dict(torch.load('./models/mlp_model_22_classes_1000.pth'))\n",
    "my_model.eval()\n",
    "\n",
    "value = \"1 0 + 3 - 6\" # Строка для создания массива картинок\n",
    "end = \"\"\n",
    "for i in value.split(\" \"):\n",
    "    (X_train, y_train,class_idx) = GetItem(dict_folders[i]) # Получаем текущую картинку \n",
    "    result = my_model(X_train) # Предсказываем поведение\n",
    "    temp = list(dict_folders.keys())[list(dict_folders.values()).index(int(torch.argmax(result)))] # Вычлисляем действительное значение данной модели\n",
    "    end += temp # Сохраняем\n",
    "    print(temp, end=\"\") # Выводим\n",
    "\n",
    "print(f\"={eval(end)}\") # Расчет значения (не будет работать для * и букв)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
